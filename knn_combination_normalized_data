import itertools
import pandas as pd #It is necessary to install pandas.
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import  GridSearchCV
from sklearn.preprocessing import StandardScaler

#Load database
ds = pd.read_excel('dados.xlsx') #download from https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant

# Separate the independent variables (features) from the dependent variable (target)
x = ds.iloc[:, :-1]
y = ds.iloc[:, -1]

resultsavg = [] #List to store the results of all tests.
results = [] #List to store the results for each mean of the n tests.

k_range = list(range(1, 21))# k varies from 1 to 21

best_models = {}  #Dictionary to store the best model for each n.

#Create an instance of StandardScaler.
scaler = StandardScaler()
#Apply statistical normalization to the input data.
x_normalized = scaler.fit_transform(x)
#Standardize the data.
x = pd.DataFrame(x_normalized, columns=x.columns)

#Set the hyperparameters for optimization.
param_grid= {
    'n_neighbors': k_range,  # Values to be tested for the number of neighbors.
    'weights': ['uniform', 'distance'],  #Values to be tested for the parameter weights.
    #‘distance’ The weights are calculated as the inverse of the distance.
    # ‘uniform’ All points in each neighborhood have equal weights.
    'p': [1, 2, 3]  #distance calculation
    #p=1 -> manhattan_distance ,
    #p=2 -> euclidean_distance
    #p=3 ->Minkowski (l_3)
}   

#Create the KNN regression model.
model = KNeighborsRegressor()

# Columns for combination.
columns_combination = ['AT', 'V', 'AP', 'RH']

# Generate combinations between the column names.
for i in range(1,5):
    comb_columns = list(itertools.combinations(columns_combination, i))
    for comb in comb_columns:
        resultsaux = []
        X = x[list(comb)].copy()
        for n in range(1, 2): 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=n)
            # Perform grid search with cross-validation and score (MAE)
            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            # Calculate the MAPE manually after cross-validation.
            y_pred = best_model.predict(X_test)
            r2 = r2_score(y_test, y_pred) #save R2
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))#save RMSE
            mape=mean_absolute_percentage_error(y_test, y_pred)#save MAPE
            mae=mean_absolute_error(y_test, y_pred)#save MAE
            resultsaux.append({ 
                'random_state': n,
                'Index': list(comb),
                'R²': r2,
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            })
            results.append({
                'random_state': n,
                'Index': list(comb),
                'R²': r2,
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            })
        
        df_results_aux = pd.DataFrame(resultsaux).set_index('Index')

        resultsavg.append({
            'Index': comb,
            'R²': df_results_aux['R²'].mean(),
            'MAE': df_results_aux['MAE'].mean(),
            'RMSE': df_results_aux['RMSE'].mean(),
            'MAPE': df_results_aux['MAPE'].mean()
        })

        
df_results = pd.DataFrame(results).set_index('Index')
file_name = 'results_knn_combination.xlsx'  # Excel file name
df_results.to_excel(file_name) # Save the DataFrame to the Excel file.
df_results_avg = pd.DataFrame(resultsavg).set_index('Index')
file_name = 'results_knn_combination_average.xlsx'  # Excel file name
df_results_avg.to_excel(file_name)  # Save the DataFrame to the Excel file

