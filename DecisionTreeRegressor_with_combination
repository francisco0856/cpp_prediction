import pandas as pd
from sklearn.tree import DecisionTreeRegressor #It is necessary to install pandas.
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
import graphviz #It is necessary to install graphviz.
from sklearn import tree
import os
import numpy as np
from sklearn.metrics import mean_absolute_error
import itertools

#Load database
ds = pd.read_excel('database.xlsx') #download from https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant

#Separate the independent variables (features) from the dependent variable (target).)
x = ds.iloc[:, :-1]
y = ds.iloc[:, -1]
'''Here, values for ccp_alpha should be selected from the Excel file created by 
the calculate_ccp_alphas_values algorithm. In the article, we used 80 with better 
R2 performance, and 20 others with scattered values for greater generalization.'''
df_alphas = pd.read_excel('ccp_alpha_selected.xlsx')
ccp_alpha = df_alphas['ccp_alpha'].values

# Split the dataset into training and testing
resultsavg = [] #List to store the results of all tests.
results = [] #List to store the results for each mean of the n tests.
best_models = {}  # Dictionary to store the best model for each n.

# Columns for combination.
columns_combination = ['AT', 'V', 'AP', 'RH']
#define the model as a regression tree.
model = DecisionTreeRegressor()

for i in range(1,5):
    comb_columns = list(itertools.combinations(columns_combination, i))
    # Loop to print the combinations.
    for comb in comb_columns:
        resultsaux = []
        X = x[list(comb)].copy()
        # Loop to train and save the best model for each value of n.
        for n in range(1, 101):     
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=n)
            # Create the regression tree model with the current value of n.
            # Set the parameters for grid search.
            param_grid = {'ccp_alpha': ccp_alpha}
            # Perform grid search with cross-validation
            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')
            grid_search.fit(X_train, y_train)
            #Get the best ccp_alpha value and the corresponding model.
            best_ccp_alpha = grid_search.best_params_['ccp_alpha']
            best_model = grid_search.best_estimator_
            #Calculate MAPE manually after cross-validation.
            y_pred = best_model.predict(X_test)
            r2 = r2_score(y_test, y_pred) #save r2 score
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))#save RMSE score
            mape=mean_absolute_percentage_error(y_test, y_pred)#save MAPE score
            mae=mean_absolute_error(y_test, y_pred) #save MAE score
                      feature_importance = best_model.feature_importances_
            resultsaux.append({ 
                'Random_state': n,
                'Index': list(comb),
                'feature_importance': feature_importance,
                'ccp_alpha': best_ccp_alpha,
                'R²': r2,
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            })
            results.append({
                'random_state': n,
                'Index': list(comb),
                'feature_importance[AT, V, AP, RH]': feature_importance,
                'ccp_alpha': best_ccp_alpha,
                'R²': r2,
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape
            })
            # Save the best model for each value of n.
            best_models[n] = (best_ccp_alpha, best_model)
        
        df_results_aux = pd.DataFrame(resultsaux).set_index('Index')

        resultsavg.append({
            'Index': comb,
            'ccp_alpha':df_results_aux['ccp_alpha'].mean(),
            'R²': df_results_aux['R²'].mean(),
            'MAE': df_results_aux['MAE'].mean(),
            'RMSE': df_results_aux['RMSE'].mean(),
            'MAPE': df_results_aux['MAPE'].mean()
        })

all_results = pd.DataFrame(results).set_index('Index')
file_name = 'results_tree_combination.xlsx'  # Excel file name
all_results.to_excel(file_name)  #  Save the DataFrame to the Excel file

df_results_avg = pd.DataFrame(resultsavg).set_index('Index')
file_name = 'results_tree_combination_average.xlsx'   # Excel file name
df_results_avg.to_excel(file_name)  # Save the DataFrame to the Excel file
folder_name_to_save = "tree"

# Loop to save the best models as high-quality PNG files
# depth_from_3_to_5
for depth in range(3, 6):
    for n, (best_ccp_alpha, best_model) in best_models.items():
        dot_data = tree.export_graphviz(best_model, out_file=None, feature_names=X.columns, filled=True, impurity=False, node_ids=False, max_depth=depth, proportion=True)
        graph = graphviz.Source(dot_data)
        graph.format = 'png'
        graph.render(f'arvore_{n}_{depth}')
        file_path = os.path.join(folder_name_to_save, f'arvore_{n}_{i}.png')
        graph.render(filename=file_path, cleanup=True)

